import os

from datetime import datetime
import requests
import strip
from bs4 import BeautifulSoup
import html2text
import re
import urllib3
from urllib.parse import urljoin, urlparse
from hashlib import md5

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class VBulletinImageDownloader:
    def __init__(self, base_url):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Referer': base_url
        }
        self.processed_messages = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π

    def normalize_url(self, img_url):
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø—É—Ç–∏ vBulletin"""
        if not img_url:
            return None

        if img_url.startswith(('http://', 'https://')):
            return img_url

        if img_url.startswith(('./', 'filedata/', 'core/')):
            base = urlparse(self.base_url)
            return f"{base.scheme}://{base.netloc}/{img_url.lstrip('./')}"

        return urljoin(self.base_url, img_url)

    def download_image(self, img_url, folder="images"):
        img_url = self.normalize_url(img_url)
        if not img_url:
            return None

        try:
            os.makedirs(folder, exist_ok=True)

            parsed = urlparse(img_url)
            filename = parsed.path.split('/')[-1] or f"image_{hash(img_url)}.jpg"

            if 'filedata/fetch' in img_url:
                params = dict(p.split('=') for p in parsed.query.split('&'))
                filename = f"img_{params.get('id', '0')}_{params.get('d', '0')}.jpg"

            filepath = os.path.join(folder, filename)

            print(f"\nüîç –ó–∞–≥—Ä—É–∂–∞–µ–º: {img_url}")
            print(f"üìÅ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫: {filepath}")

            response = self.session.get(img_url, stream=True, verify=False, timeout=15)
            response.raise_for_status()

            if not response.headers.get('Content-Type', '').startswith('image/'):
                print(f"‚ùå –ù–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (Content-Type: {response.headers.get('Content-Type')})")
                return None

            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(1024):
                    f.write(chunk)

            print("‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")
            return filepath

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
            return None


def convert_vbulletin_to_markdown(url):
    print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–º—ã...")

    downloader = VBulletinImageDownloader(url)

    try:
        print(f"\nüåê –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
        response = downloader.session.get(url, verify=False, timeout=15)
        response.raise_for_status()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {str(e)}")
        return

    soup = BeautifulSoup(response.text, 'html.parser')
    title = soup.find('h1').get_text(strip=True) if soup.find('h1') else "Untitled"
    print(f"\nüìå –¢–µ–º–∞: {title}")

    # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
    safe_title = re.sub(r'[^\w\-_\. ]', '_', title)[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∏ –∑–∞–º–µ–Ω—è–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
    # output_file = f"{safe_title}.md"

    current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    output_file = f"{current_time}_{safe_title}.md"



    # –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å—É —Å–æ–æ–±—â–µ–Ω–∏—è
    posts = soup.find_all('div', class_=re.compile(r'b-post__content js-post__content'))

    if not posts:
        posts = soup.find_all('div', class_='b-post__content')

    print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(posts)}")

    h = html2text.HTML2Text()
    h.ignore_links = False
    h.body_width = 0

    markdown_content = f"# {title}\n\n"
    image_counter = 0

    for i, post in enumerate(posts, 1):
        author_tag = post.find_previous('a', class_=re.compile(r'username|author'))
        author = author_tag.get_text(strip=True) if author_tag else "Anonymous"

        date_tag = post.find_previous('span', class_=re.compile(r'date|time'))
        date = date_tag.get_text(strip=True) if date_tag else "Unknown date"

        print(f"\nüìù –°–æ–æ–±—â–µ–Ω–∏–µ {i}/{len(posts)} –æ—Ç {author} ({date})")

        for img in post.find_all('img'):
            img_url = img.get('src') or img.get('data-src')
            if not img_url:
                continue

            local_path = downloader.download_image(img_url)
            if local_path:
                image_counter += 1
                img['src'] = local_path
                print(f"üñºÔ∏è [–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {image_counter}] –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {local_path}")

        post_md = h.handle(str(post))
        markdown_content += f"## {author} ({date})\n\n{post_md}\n\n---\n\n"

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(markdown_content)

    print(f"\nüéâ –ì–æ—Ç–æ–≤–æ!")
    print(f"üìÑ –¢–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: {output_file}")
    print(f"üìä –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(posts)}")
    print(f"üñºÔ∏è –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {image_counter}")
    print(f"üìÅ –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: {os.path.abspath('../images')}")


if __name__ == "__main__":
    url = input("–í–≤–µ–¥–∏—Ç–µ URL —Ç–µ–º—ã vBulletin: ").strip()
    convert_vbulletin_to_markdown(url)
